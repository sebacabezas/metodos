abline(regresion1$coefficients[[1]], regresion1$coefficients[[2]])
ECM1 = c(0)
ECM2 = c(mean((rep(datos$y[1],length(datos$yval)) - datos$yval)^2))
for (i in 2:length(datos$X)) {
x1 = datos$X[1:i]
y1 = datos$y[1:i]
reg = lm(y1~x1)
pred1 = predict(reg, data.frame("x1" = datos$X[1:i]))
pred2 = predict(reg, newdata = data.frame("x1" = datos$Xval))
ECM1 = c(ECM1, mean((pred1 - datos$y[1:i])^2))
ECM2 = c(ECM2, mean((pred2 - datos$yval)^2))
}
plot(seq(1,12),  ECM1, xlim = c(-0,16), ylim = c(0,300))
lines(ECM2)
base_potencia_8 = poly(rbind(datos$X, datos$Xval, datos$Xtest), degree = 8, raw = TRUE, simple = TRUE)
c1 = colSums(base_potencia_8)
media_por_cols = apply(base_potencia_8,2, mean)
var_por_cols = apply(base_potencia_8,2,var)
base_potencia_8 = scale(base_potencia_8, center = FALSE, scale = c1)
regresion_4 = lm(datos$y ~ poly(datos$X, degree = 8, raw = TRUE, simple = TRUE))
grilla = seq(-50,50,length=100000)
plot(datos$X, datos$y, ylim = c(-60,60))
lines(grilla, regresion_4$coefficients[[1]] + regresion_4$coefficients[[2]]*grilla + regresion_4$coefficients[[3]]*grilla^2 +
regresion_4$coefficients[[4]]*grilla^3 + regresion_4$coefficients[[5]]*grilla^4 +
regresion_4$coefficients[[6]]*grilla^5 + regresion_4$coefficients[[7]]*grilla^6 +
regresion_4$coefficients[[8]]*grilla^7 + regresion_4$coefficients[[9]]*grilla^8, col = "green")
base_poly = as.data.frame(poly(datos$X, degree = 8, raw = TRUE, simple = TRUE))
base_poly = cbind(datos$y, base_poly)
colnames(base_poly) = c("y","x11","x12","x13","x14","x15","x16","x17","x18")
base_poly_2 = as.data.frame(poly(datos$Xval, degree = 8, raw = TRUE, simple = TRUE))
colnames(base_poly_2) = c("x11","x12","x13","x14","x15","x16","x17","x18")
ECM1 = c(0)
ECM2 = c(mean((rep(datos$y[1],length(datos$yval)) - datos$yval)^2))
for (i in 2:length(datos$X)) {
data_temp = base_poly[1:i,]
reg = lm(y~., data = data_temp)
pred1 = predict(reg, base_poly[1:(i),])
pred2 = predict(reg, base_poly_2)
ECM1 = c(ECM1, mean((pred1 - datos$y[1:i])^2))
ECM2 = c(ECM2, mean((pred2 - datos$yval)^2))
}
plot(seq(1,12),  ECM1, xlim = c(0,12), ylim = c(0,250))
lines(ECM2)
# Lambda = 10
modelo_ridge <- lm.ridge( y~ ., data=base_poly, lambda = 10/24)
grilla = seq(-50,50,length=1000)
plot(datos$X, datos$y)
lines(grilla, coef(modelo_ridge)[[1]] + coef(modelo_ridge)[[2]]*grilla + coef(modelo_ridge)[[3]]*grilla^2 +
coef(modelo_ridge)[[4]]*grilla^3 + coef(modelo_ridge)[[5]]*grilla^4 +
coef(modelo_ridge)[[6]]*grilla^5 + coef(modelo_ridge)[[7]]*grilla^6 +
coef(modelo_ridge)[[8]]*grilla^7 + coef(modelo_ridge)[[9]]*grilla^8, col = "green")
# Lambda = 100
modelo_ridge2 <- lm.ridge( y~ ., data=base_poly, lambda = 100/28)
grilla = seq(-50,50,length=1000)
plot(datos$X, datos$y)
lines(grilla, coef(modelo_ridge2)[[1]] + coef(modelo_ridge2)[[2]]*grilla + coef(modelo_ridge2)[[3]]*grilla^2 +
coef(modelo_ridge2)[[4]]*grilla^3 + coef(modelo_ridge2)[[5]]*grilla^4 +
coef(modelo_ridge2)[[6]]*grilla^5 + coef(modelo_ridge2)[[7]]*grilla^6 +
coef(modelo_ridge2)[[8]]*grilla^7 + coef(modelo_ridge2)[[9]]*grilla^8, col = "green")
data_ml1 = data.frame("y" = datos$y, "x" = datos$X)
data_testeo = data.frame("y" = datos$ytest, "x" = datos$Xtest)
ml1 = lm(y~x, data = data_ml1)
prediccion_mod1 = predict(ml1, data_testeo)
ECM_mod1 = mean((prediccion_mod1-datos$ytest)^2)
base_poly_test = poly(datos$Xtest, degree = 8, raw = TRUE, simple = TRUE)
ml2 = lm(y~., data = base_poly)
prediccion_mod2 = predict(ml2, base_poly_2)
mean((prediccion_mod2-datos$ytest)^2)
modelo_ridge <- lm.ridge( y~ ., data=base_poly, lambda = 10/28)
prediccion_ridge = as.matrix(cbind(rep(1,length(datos$Xtest)),as.matrix(base_poly_test[,]))) %*% coef(modelo_ridge)
ECM_ridge = mean((prediccion_ridge-datos$ytest)^2)
ECM_ridge
ml2
base_poly_2
ml2
prediccion_mod2
datos$ytest
mean((prediccion_mod2-datos$yval)^2)
mean((prediccion_mod2-datos$yval)^2)
prediccion_mod2
plot(test_mse_lasso)
set.seed(10)
base <- read_csv("FINAL_USO.csv")
cant_datos = dim(base)[1]
cant_datos*0.6
colnames(base)
indice = seq(1,1718)
indice_entrenamiento = sample(indice, 1032,replace = FALSE)
indice = indice[-indice_entrenamiento]
indice2 = seq(1,length(indice))
indice2 = sample(indice2, 343, replace = FALSE)
indice_val = indice[indice2]
indice_test = indice[-indice2]
base = base[,c(6:dim(base)[2])]
colnames(base)[1] = "Adj_close"
base_entrenamiento = base[indice_entrenamiento,]
base_testeo = base[indice_test,]
ridge = lm.ridge( Adj_close~ . , data=base_entrenamiento, lambda = exp(c(1,2,3,4,5,6,7,8,9,10,12,13)))
plot(ridge, type = "l")
x_train <- model.matrix(Adj_close~., data = base_entrenamiento)[, -1]
y_train <- base_entrenamiento$Adj_close
x_test = model.matrix(Adj_close~., data = base_testeo)[, -1]
y_test = base_testeo$Adj_close
modelo_ridge <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 0,
lambda     = seq(1,100,0.5),
standardize = TRUE
)
regularizacion <- modelo_ridge$beta %>%
as.matrix() %>%
t() %>%
as_tibble() %>%
mutate(lambda = modelo_ridge$lambda)
regularizacion <- regularizacion %>%
pivot_longer(
cols = !lambda,
names_to = "predictor",
values_to = "coeficientes"
)
regularizacion %>%
ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
geom_line() +
scale_x_log10(
breaks = trans_breaks("log10", function(x) 10^x),
labels = trans_format("log10", math_format(10^.x))
) +
labs(title = "Coeficientes del modelo en función de la regularización") +
theme_bw() +
theme(legend.position = "none")
x_train <- model.matrix(Adj_close~., data = base_entrenamiento)[, -1]
y_train <- base_entrenamiento$Adj_close
modelo_lasso <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 1,
lambda     = seq(1,50,0.5),
standardize = TRUE
)
regularizacion <- modelo_lasso$beta %>%
as.matrix() %>%
t() %>%
as_tibble() %>%
mutate(lambda = modelo_lasso$lambda)
regularizacion <- regularizacion %>%
pivot_longer(
cols = !lambda,
names_to = "predictor",
values_to = "coeficientes"
)
regularizacion %>%
ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
geom_line() +
scale_x_log10(
breaks = trans_breaks("log10", function(x) 10^x),
labels = trans_format("log10", math_format(10^.x))
) +
labs(title = "Coeficientes del modelo en función de la regularización") +
theme_bw() +
theme(legend.position = "none")
cv_error <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 0,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error)
paste("Mejor valor de lambda encontrado:", cv_error$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error$lambda.1se)
cv_error2 <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error2)
paste("Mejor valor de lambda encontrado:", cv_error2$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error2$lambda.1se)
modelo_ridge_final = glmnet(
x           = x_train,
y           = y_train,
alpha       = 0,
lambda     = cv_error$lambda.min,
standardize = TRUE
)
predicciones_test <- predict(modelo_ridge_final, newx = x_test)
test_mse_ridge <- mean((predicciones_test - y_test)^2)
paste("Error (mse) de test:", test_mse_ridge)
modelo_lasso_final = glmnet(
x           = x_train,
y           = y_train,
alpha       = 1,
lambda     = cv_error2$lambda.min,
standardize = TRUE
)
predicciones_test2 <- predict(modelo_lasso_final, newx = x_test)
test_mse_lasso <- mean((predicciones_test2 - y_test)^2)
paste("Error (mse) de test:", test_mse_lasso)
plot(test_mse_lasso)
plot(predicciones_test2)
plot(y_test)
lines(predicciones_test2)
datos <- readMat("represa-1.mat")
regresion1 = lm(datos$y~datos$X)
plot(datos$X,datos$y)
abline(regresion1$coefficients[[1]], regresion1$coefficients[[2]])
ECM1 = c(0)
ECM2 = c(mean((rep(datos$y[1],length(datos$yval)) - datos$yval)^2))
for (i in 2:length(datos$X)) {
x1 = datos$X[1:i]
y1 = datos$y[1:i]
reg = lm(y1~x1)
pred1 = predict(reg, data.frame("x1" = datos$X[1:i]))
pred2 = predict(reg, newdata = data.frame("x1" = datos$Xval))
ECM1 = c(ECM1, mean((pred1 - datos$y[1:i])^2))
ECM2 = c(ECM2, mean((pred2 - datos$yval)^2))
}
plot(seq(1,12),  ECM1, xlim = c(-0,16), ylim = c(0,300))
lines(ECM2)
base_potencia_8 = poly(rbind(datos$X, datos$Xval, datos$Xtest), degree = 8, raw = TRUE, simple = TRUE)
c1 = colSums(base_potencia_8)
media_por_cols = apply(base_potencia_8,2, mean)
var_por_cols = apply(base_potencia_8,2,var)
base_potencia_8 = scale(base_potencia_8, center = FALSE, scale = c1)
regresion_4 = lm(datos$y ~ poly(datos$X, degree = 8, raw = TRUE, simple = TRUE))
grilla = seq(-50,50,length=100000)
plot(datos$X, datos$y, ylim = c(-60,60))
lines(grilla, regresion_4$coefficients[[1]] + regresion_4$coefficients[[2]]*grilla + regresion_4$coefficients[[3]]*grilla^2 +
regresion_4$coefficients[[4]]*grilla^3 + regresion_4$coefficients[[5]]*grilla^4 +
regresion_4$coefficients[[6]]*grilla^5 + regresion_4$coefficients[[7]]*grilla^6 +
regresion_4$coefficients[[8]]*grilla^7 + regresion_4$coefficients[[9]]*grilla^8, col = "green")
base_poly = as.data.frame(poly(datos$X, degree = 8, raw = TRUE, simple = TRUE))
base_poly = cbind(datos$y, base_poly)
colnames(base_poly) = c("y","x11","x12","x13","x14","x15","x16","x17","x18")
base_poly_2 = as.data.frame(poly(datos$Xval, degree = 8, raw = TRUE, simple = TRUE))
colnames(base_poly_2) = c("x11","x12","x13","x14","x15","x16","x17","x18")
ECM1 = c(0)
ECM2 = c(mean((rep(datos$y[1],length(datos$yval)) - datos$yval)^2))
for (i in 2:length(datos$X)) {
data_temp = base_poly[1:i,]
reg = lm(y~., data = data_temp)
pred1 = predict(reg, base_poly[1:(i),])
pred2 = predict(reg, base_poly_2)
ECM1 = c(ECM1, mean((pred1 - datos$y[1:i])^2))
ECM2 = c(ECM2, mean((pred2 - datos$yval)^2))
}
plot(seq(1,12),  ECM1, xlim = c(0,12), ylim = c(0,250))
lines(ECM2)
plot(seq(1,12),  ECM1, xlim = c(0,12), ylim = c(0,250), xlab = "valores")
lines(ECM2)
# Lambda = 10
modelo_ridge <- lm.ridge( y~ ., data=base_poly, lambda = 10/24)
grilla = seq(-50,50,length=1000)
plot(datos$X, datos$y)
lines(grilla, coef(modelo_ridge)[[1]] + coef(modelo_ridge)[[2]]*grilla + coef(modelo_ridge)[[3]]*grilla^2 +
coef(modelo_ridge)[[4]]*grilla^3 + coef(modelo_ridge)[[5]]*grilla^4 +
coef(modelo_ridge)[[6]]*grilla^5 + coef(modelo_ridge)[[7]]*grilla^6 +
coef(modelo_ridge)[[8]]*grilla^7 + coef(modelo_ridge)[[9]]*grilla^8, col = "green")
plot(datos$X, datos$y, xlab = "Valores de x", ylab = "Valores de Y", main = "lambda = 10")
lines(grilla, coef(modelo_ridge)[[1]] + coef(modelo_ridge)[[2]]*grilla + coef(modelo_ridge)[[3]]*grilla^2 +
coef(modelo_ridge)[[4]]*grilla^3 + coef(modelo_ridge)[[5]]*grilla^4 +
coef(modelo_ridge)[[6]]*grilla^5 + coef(modelo_ridge)[[7]]*grilla^6 +
coef(modelo_ridge)[[8]]*grilla^7 + coef(modelo_ridge)[[9]]*grilla^8, col = "green")
# Lambda = 100
modelo_ridge2 <- lm.ridge( y~ ., data=base_poly, lambda = 100/28)
grilla = seq(-50,50,length=1000)
plot(datos$X, datos$y, xlab = "Valores de x", ylab = "Valores de Y", main = "lambda = 100")
lines(grilla, coef(modelo_ridge2)[[1]] + coef(modelo_ridge2)[[2]]*grilla + coef(modelo_ridge2)[[3]]*grilla^2 +
coef(modelo_ridge2)[[4]]*grilla^3 + coef(modelo_ridge2)[[5]]*grilla^4 +
coef(modelo_ridge2)[[6]]*grilla^5 + coef(modelo_ridge2)[[7]]*grilla^6 +
coef(modelo_ridge2)[[8]]*grilla^7 + coef(modelo_ridge2)[[9]]*grilla^8, col = "green")
data_ml1 = data.frame("y" = datos$y, "x" = datos$X)
data_testeo = data.frame("y" = datos$ytest, "x" = datos$Xtest)
ml1 = lm(y~x, data = data_ml1)
prediccion_mod1 = predict(ml1, data_testeo)
ECM_mod1 = mean((prediccion_mod1-datos$ytest)^2)
ECM_mod1
base_poly_test = poly(datos$Xtest, degree = 8, raw = TRUE, simple = TRUE)
ml2 = lm(y~., data = base_poly)
prediccion_mod2 = predict(ml2, base_poly_2)
mean((prediccion_mod2-datos$yval)^2)
modelo_ridge <- lm.ridge( y~ ., data=base_poly, lambda = 10/28)
prediccion_ridge = as.matrix(cbind(rep(1,length(datos$Xtest)),as.matrix(base_poly_test[,]))) %*% coef(modelo_ridge)
ECM_ridge = mean((prediccion_ridge-datos$ytest)^2)
ECM_ridge
set.seed(10)
base <- read_csv("FINAL_USO.csv")
cant_datos = dim(base)[1]
cant_datos*0.6
colnames(base)
indice = seq(1,1718)
indice_entrenamiento = sample(indice, 1032,replace = FALSE)
indice = indice[-indice_entrenamiento]
indice2 = seq(1,length(indice))
indice2 = sample(indice2, 343, replace = FALSE)
indice_val = indice[indice2]
indice_test = indice[-indice2]
base = base[,c(6:dim(base)[2])]
colnames(base)[1] = "Adj_close"
base_entrenamiento = base[indice_entrenamiento,]
base_testeo = base[indice_test,]
ridge = lm.ridge( Adj_close~ . , data=base_entrenamiento, lambda = exp(c(1,2,3,4,5,6,7,8,9,10,12,13)))
plot(ridge, type = "l")
base = base[,c(6:dim(base)[2])]
colnames(base)[1] = "Adj_close"
base_entrenamiento = base[indice_entrenamiento,]
base_testeo = base[indice_test,]
ridge = lm.ridge( Adj_close~ . , data=base_entrenamiento, lambda = exp(c(1,2,3,4,5,6,7,8,9,10,12,13)))
plot(ridge, type = "l")
x_train <- model.matrix(Adj_close~., data = base_entrenamiento)[, -1]
y_train <- base_entrenamiento$Adj_close
x_test = model.matrix(Adj_close~., data = base_testeo)[, -1]
y_test = base_testeo$Adj_close
modelo_ridge <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 0,
lambda     = seq(1,100,0.5),
standardize = TRUE
)
regularizacion <- modelo_ridge$beta %>%
as.matrix() %>%
t() %>%
as_tibble() %>%
mutate(lambda = modelo_ridge$lambda)
regularizacion <- regularizacion %>%
pivot_longer(
cols = !lambda,
names_to = "predictor",
values_to = "coeficientes"
)
regularizacion %>%
ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
geom_line() +
scale_x_log10(
breaks = trans_breaks("log10", function(x) 10^x),
labels = trans_format("log10", math_format(10^.x))
) +
labs(title = "Coeficientes del modelo en función de la regularización") +
theme_bw() +
theme(legend.position = "none")
x_train <- model.matrix(Adj_close~., data = base_entrenamiento)[, -1]
x_train <- model.matrix(Adj_close~., data = base_entrenamiento)[, -1]
y_train <- base_entrenamiento$Adj_close
modelo_lasso <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 1,
lambda     = seq(1,50,0.5),
standardize = TRUE
)
regularizacion <- modelo_lasso$beta %>%
as.matrix() %>%
t() %>%
as_tibble() %>%
mutate(lambda = modelo_lasso$lambda)
regularizacion <- regularizacion %>%
pivot_longer(
cols = !lambda,
names_to = "predictor",
values_to = "coeficientes"
)
regularizacion %>%
ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
geom_line() +
scale_x_log10(
breaks = trans_breaks("log10", function(x) 10^x),
labels = trans_format("log10", math_format(10^.x))
) +
labs(title = "Coeficientes del modelo en función de la regularización") +
theme_bw() +
theme(legend.position = "none")
cv_error <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 0,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error)
x_train <- model.matrix(Adj_close~., data = base_entrenamiento)[, -1]
y_train <- base_entrenamiento$Adj_close
modelo_lasso <- glmnet(
x           = x_train,
y           = y_train,
alpha       = 1,
lambda     = seq(1,50,0.5),
standardize = TRUE
)
regularizacion <- modelo_lasso$beta %>%
as.matrix() %>%
t() %>%
as_tibble() %>%
mutate(lambda = modelo_lasso$lambda)
regularizacion <- regularizacion %>%
pivot_longer(
cols = !lambda,
names_to = "predictor",
values_to = "coeficientes"
)
regularizacion %>%
ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
geom_line() +
scale_x_log10(
breaks = trans_breaks("log10", function(x) 10^x),
labels = trans_format("log10", math_format(10^.x))
) +
labs(title = "Coeficientes del modelo en función de la regularización") +
theme_bw() +
theme(legend.position = "none")
cv_error <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 0,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error)
paste("Mejor valor de lambda encontrado:", cv_error$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error$lambda.1se)
cv_error2 <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error2)
paste("Mejor valor de lambda encontrado:", cv_error2$lambda.min)
cv_error2 <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error2)
paste("Mejor valor de lambda encontrado:", cv_error2$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error2$lambda.1se)
modelo_ridge_final = glmnet(
x           = x_train,
y           = y_train,
alpha       = 0,
lambda     = cv_error$lambda.min,
standardize = TRUE
)
predicciones_test <- predict(modelo_ridge_final, newx = x_test)
test_mse_ridge <- mean((predicciones_test - y_test)^2)
paste("Error (mse) de test:", test_mse_ridge)
cv_error <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 0,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error)
paste("Mejor valor de lambda encontrado:", cv_error$lambda.min)
cv_error2 <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error2)
paste("Mejor valor de lambda encontrado:", cv_error2$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error2$lambda.1se)
cv_error2 <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1,
nfolds = 10,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error2)
paste("Mejor valor de lambda encontrado:", cv_error2$lambda.min)
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error2$lambda.1se)
modelo_ridge_final = glmnet(
x           = x_train,
y           = y_train,
alpha       = 0,
lambda     = cv_error$lambda.min,
standardize = TRUE
)
predicciones_test <- predict(modelo_ridge_final, newx = x_test)
test_mse_ridge <- mean((predicciones_test - y_test)^2)
paste("Error (mse) de test:", test_mse_ridge)
modelo_lasso_final = glmnet(
x           = x_train,
y           = y_train,
alpha       = 1,
lambda     = cv_error2$lambda.min,
standardize = TRUE
)
predicciones_test2 <- predict(modelo_lasso_final, newx = x_test)
test_mse_lasso <- mean((predicciones_test2 - y_test)^2)
paste("Error (mse) de test:", test_mse_lasso)
plot(y_test)
lines(predicciones_test2)
